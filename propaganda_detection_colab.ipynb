{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YMJA fine grained propoganda analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEAWlRpRvQjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install packages\n",
        "\n",
        "!pip install torch\n",
        "#install nvidia apex\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "!pip install --user --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" apex/\n",
        "!pip install pytorch-pretrained-bert --user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcQ4SVcFtUu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download dataset\n",
        "\n",
        "!wget https://propaganda.qcri.org/nlp4if-shared-task/data/datasets.tgz\n",
        "!tar xfz datasets.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gskKnVkQ2ihd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import pkg_resources\n",
        "import seaborn as sns\n",
        "import time\n",
        "import scipy.stats as stats\n",
        "import gc\n",
        "import re\n",
        "import operator \n",
        "import sys\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import os\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "import warnings\n",
        "warnings.filterwarnings(action='once')\n",
        "import pickle\n",
        "from apex import amp\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import json,requests\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZWHl5FP7Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import os.path\n",
        "import numpy as np\n",
        "import random\n",
        "import sys\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n",
        "\n",
        "# This is the Bert configuration file\n",
        "from pytorch_pretrained_bert import BertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqdvE1fl3mTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device=torch.device('cuda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBDc39cZQaNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read Functions\n",
        "\n",
        "def read_articles_from_file_list(folder_name, file_pattern=\"*.txt\"):\n",
        "    \"\"\"\n",
        "    Read articles from files matching patterns <file_pattern> from  \n",
        "    the directory <folder_name>. \n",
        "    The content of the article is saved in the array <sentence_list>.\n",
        "    Each element of <sentence_list> is one line of the article.\n",
        "    Two additional arrays are created: <sentence_id_list> and\n",
        "    <article_id_list>, holding the id of the sentences and the article.\n",
        "    The arrays <article_id_list> and <sentence_id_list> are the first\n",
        "    two columns of the predictions for the article, i.e. the format\n",
        "    of the file <dev_template_labels_file>, they will be used to match\n",
        "    the sentences with their gold labels in <train_labels_folder> \n",
        "    or <dev_template_labels_file>.\n",
        "    \"\"\"\n",
        "    file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
        "    article_id_list, sentence_id_list, sentence_list = ([], [], [])\n",
        "    offsets = []\n",
        "    start = 0\n",
        "    end = 0\n",
        "    article_lens = {}\n",
        "    for filename in sorted(file_list):\n",
        "        article_id = os.path.basename(filename).split(\".\")[0][7:]\n",
        "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "            for sentence_id, row in enumerate(f.readlines(), 1):\n",
        "                sentence_list.append(row.rstrip())\n",
        "                article_id_list.append(article_id)\n",
        "                sentence_id_list.append(str(sentence_id))\n",
        "                end = start + len(row)\n",
        "                offsets.append([start, end])\n",
        "                start = end\n",
        "        article_lens[article_id] = end\n",
        "    return article_id_list, sentence_id_list, sentence_list, offsets, article_lens\n",
        "\n",
        "\n",
        "def are_ids_aligned(article_id_list, sentence_id_list, \n",
        "                    reference_article_id_list, reference_sentence_id_list):\n",
        "    \"\"\"\n",
        "    check whether the two lists of ids of the articles and the sentences are aligned\n",
        "    \"\"\"\n",
        "    for art, ref_art, sent, ref_sent in zip(article_id_list, reference_article_id_list, \n",
        "                                            sentence_id_list, reference_sentence_id_list):\n",
        "        if art != ref_art:\n",
        "            print(\"ERROR: article ids do not match: article id = %s, reference article id = %s\"%(art, ref_art))\n",
        "            return False\n",
        "        if sent != ref_sent:\n",
        "            print(\"ERROR: sentence ids do not match: article id:%s,%s sentence id:%s,%s\" %(art, ref_art, sent, ref_sent))\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def read_predictions_from_file(filename):\n",
        "    \"\"\"\n",
        "    Reader for the gold file and the template output file. \n",
        "    Return values are three arrays with article ids, sentence ids and labels \n",
        "    (or ? in the case of a template file). For more info on the three \n",
        "    arrays see comments in function read_articles_from_file_list()\n",
        "    \"\"\"\n",
        "    articles_id, sentence_id_list, gold_labels = ([], [], [])\n",
        "    with open(filename, \"r\") as f:\n",
        "        for row in f.readlines():\n",
        "            article_id, sentence_id, gold_label = row.rstrip().split(\"\\t\")\n",
        "            articles_id.append(article_id)\n",
        "            sentence_id_list.append(sentence_id)\n",
        "            gold_labels.append(gold_label)  \n",
        "      \n",
        "    return articles_id, sentence_id_list, gold_labels\n",
        "  \n",
        "  \n",
        "def read_flc_predictions_from_file(article_lens, flc_filename):\n",
        "    \"\"\"\n",
        "    Reader for the gold file and the template output file. \n",
        "    Return values are three arrays with article ids, sentence ids and labels \n",
        "    (or ? in the case of a template file). For more info on the three \n",
        "    arrays see comments in function read_articles_from_file_list()\n",
        "    \"\"\"\n",
        "    article = []\n",
        "    with open(flc_filename, \"r\") as f:\n",
        "      for row in f.readlines():\n",
        "        article_id, gold_labels, begin_offset, end_offset = row.rstrip().split(\"\\t\")\n",
        "        if len(article) == 0:\n",
        "          l = article_lens[article_id]\n",
        "          for x in range(l):\n",
        "            article.append(set())\n",
        "        for offset in range(begin_offset, end_offset):\n",
        "          for label in gold_labels.split(','):\n",
        "            article[offset].add(label)\n",
        "    return article_id, article\n",
        "\n",
        "\n",
        "def read_predictions_from_file_list(article_lens, folder_name, file_pattern, flc_file_pattern):\n",
        "    \"\"\"\n",
        "    Reader for the gold label files and the template output files\n",
        "    <folder_name> is the folder hosting the files. \n",
        "    <file_pattern> values are {\"*.task-SLC.labels\", \"*.task-SLC-template.out\"}. \n",
        "    Return values are three arrays with article ids, sentence ids and labels \n",
        "    (or ? in the case of a template file). For more info on the three \n",
        "    arrays see comments in function read_articles_from_file_list()\n",
        "    \"\"\"\n",
        "    gold_file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
        "    articles_id, sentence_id_list, gold_labels = ([], [], [])\n",
        "    for filename in sorted(gold_file_list):\n",
        "        art_ids, sent_ids, golds = read_predictions_from_file(filename)\n",
        "        articles_id += art_ids\n",
        "        sentence_id_list += sent_ids\n",
        "        gold_labels += golds\n",
        "        \n",
        "    gold_flc_file_list = glob.glob(os.path.join(folder_name, flc_file_pattern))\n",
        "    flc_labeled_articles = {}\n",
        "    for filename in sorted(gold_flc_file_list):\n",
        "        article_id, flc_labeled_article = read_flc_predictions_from_file(article_lens, filename)\n",
        "        flc_labeled_articles[article_id] = flc_labeled_article   \n",
        "    return articles_id, sentence_id_list, gold_labels, flc_labeled_articles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OHZhee15i7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split and evaluate\n",
        "\n",
        "train_folder = \"datasets/train-articles\" \n",
        "train_labels_folder = \"datasets/train-labels-SLC\"\n",
        "dev_folder = \"datasets/dev-articles\"\n",
        "test_folder = \"test\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdYXTiK1l6dG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_article_ids, train_sentence_ids, sentence_list, offsets, article_lens = read_articles_from_file_list(train_folder)\n",
        "reference_articles_id, reference_sentence_id_list, gold_labels, flc_labeled_articles = read_predictions_from_file_list(article_lens,\n",
        "                                                                                                                       train_labels_folder,                                                                                                                       \n",
        "                                                                                                                       \"*.task-SLC.labels\",\n",
        "                                                                                                                      \"*.task-FLC.labels\")\n",
        "\n",
        "dev_article_id_list, dev_sentence_id_list, dev_sentence_list, dev_offsets, dev_article_lens = read_articles_from_file_list(dev_folder)\n",
        "gold_labels = [0 if label == 'non-propaganda' else 1 for label in gold_labels]\n",
        "test_article_id_list, test_sentence_id_list, test_sentence_list, test_offsets, test_article_lens = read_articles_from_file_list(test_folder)\n",
        "\n",
        "dev_ids = [article_id + dev_sentence_id_list[ind] for ind, article_id in enumerate(dev_article_id_list)]\n",
        "test_ids = [article_id + test_sentence_id_list[ind] for ind, article_id in enumerate(test_article_id_list)]\n",
        "\n",
        "ids = [article_id + train_sentence_ids[ind] for ind, article_id in enumerate(train_article_ids)]\n",
        "refids = [article_id + reference_sentence_id_list[ind] for ind, article_id in enumerate(reference_articles_id)]\n",
        "\n",
        "dev_datadf = []\n",
        "test_datadf = []\n",
        "labeldf = []\n",
        "datadf = []\n",
        "for ind, segment_id in enumerate(dev_ids):\n",
        "  dev_datadf.append({'id': segment_id, 'text': dev_sentence_list[ind]})\n",
        "for ind, segment_id in enumerate(test_ids):\n",
        "  test_datadf.append({'id': segment_id, 'text': test_sentence_list[ind]})\n",
        "for ind, segment_id in enumerate(ids):\n",
        "  datadf.append({'id': segment_id, 'text': sentence_list[ind]})\n",
        "for ind, segment_id in enumerate(refids):\n",
        "  labeldf.append({'id': segment_id, 'label': gold_labels[ind]})\n",
        "datadf = pd.DataFrame(datadf)\n",
        "labeldf = pd.DataFrame(labeldf)\n",
        "dev_datadf = pd.DataFrame(dev_datadf)\n",
        "test_datadf = pd.DataFrame(test_datadf)\n",
        "\n",
        "\n",
        "datadf = pd.merge(datadf, labeldf, on='id', how='inner')\n",
        "datadf = datadf.drop_duplicates('id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVGYH208zu4H",
        "colab_type": "text"
      },
      "source": [
        "# Prediction based on Perspective API scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZgVyWhNs9sZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PERSPECTIVE_KEY = 'YOUR_PERSPECTIVE_API_KEY_HERE'\n",
        "ATTRIBUTES = ['TOXICITY', 'SEVERE_TOXICITY', 'IDENTITY_ATTACK', 'INSULT',\n",
        "'PROFANITY', 'THREAT', 'SEXUALLY_EXPLICIT', 'FLIRTATION',\n",
        "'INFLAMMATORY', 'OBSCENE', 'LIKELY_TO_REJECT', 'UNSUBSTANTIAL']\n",
        "\n",
        "def call_perspective_api(text, attributes, PERSPECTIVE_KEY):\n",
        "    backoff_counter = 1\n",
        "    while True:\n",
        "      path = ' https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze?key=%s' % PERSPECTIVE_KEY\n",
        "      request = {\n",
        "          'comment' : {'text' : text},\n",
        "          'requestedAttributes' : { c : {} for c in attributes},\n",
        "          'doNotStore' : True,\n",
        "      }\n",
        "      response = requests.post(path, json=request)\n",
        "      prob = {}\n",
        "      if response.status_code == 429:\n",
        "         time.sleep(10 * backoff_counter)\n",
        "         backoff_counter += 1\n",
        "      else:\n",
        "        break\n",
        "    if response.status_code == 200:\n",
        "      data = json.loads(response.text)\n",
        "      scores_simplified = {}\n",
        "      attribute_scores = data['attributeScores']\n",
        "      for attr, data in attribute_scores.items():\n",
        "          prob[attr] = data['summaryScore']['value']\n",
        "      return prob\n",
        "    else:\n",
        "      for attr in attributes:\n",
        "        prob[attr] = -1\n",
        "      return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhQfky1O0JXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = []\n",
        "for sentence in sentence_list:\n",
        "  probs = call_perspective_api(sentence, ATTRIBUTES, PERSPECTIVE_KEY)\n",
        "  X_train.append([probs[a] for a in ATTRIBUTES])\n",
        "  \n",
        "X_dev = []\n",
        "for sentence in dev_sentence_list:\n",
        "  probs = call_perspective_api(sentence, ATTRIBUTES, PERSPECTIVE_KEY)\n",
        "  X_dev.append([probs[a] for a in ATTRIBUTES])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Knp_PZlzwmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_main(train_folder, train_labels_folder, task_SLC_output_file,\n",
        "               dev_labels_folder=None, dev_labels_file=None):\n",
        "  # loading articles' content from *.txt files in the train folder\n",
        "  train_article_ids, train_sentence_ids, sentence_list = read_articles_from_file_list(train_folder)\n",
        "\n",
        "  # loading gold labels, articles ids and sentence ids from files *.task-SLC.labels in the train labels folder \n",
        "  reference_articles_id, reference_sentence_id_list, gold_labels = read_predictions_from_file_list(train_labels_folder, \"*.task-SLC.labels\")\n",
        "\n",
        "  # checking that the number of sentences in the raw training set and the gold label file\n",
        "  if not are_ids_aligned(train_article_ids, train_sentence_ids, reference_articles_id, reference_sentence_id_list):\n",
        "    sys.exit(\"Exiting: training set article ids and gold labels are not aligned\")\n",
        "  print(\"Loaded %d sentences from %d articles\" % (len(sentence_list), len(set(train_article_ids))))\n",
        "\n",
        "  # compute one feature for each sentence: the length of the sentence and train the model\n",
        "  train = np.array([ len(sentence) for sentence in sentence_list ]).reshape(-1, 1)\n",
        "  model = LogisticRegression(penalty='l2', class_weight='balanced', solver=\"lbfgs\")\n",
        "  model.fit(train, gold_labels)\n",
        "\n",
        "  # reading data from the development set\n",
        "  dev_article_id_list, dev_sentence_id_list, dev_sentence_list = read_articles_from_file_list(dev_folder)\n",
        "  if dev_labels_folder is not None:\n",
        "    assert(dev_labels_file is not None)\n",
        "    reference_articles_id, reference_sentence_id_list, dev_labels = read_predictions_from_file_list(dev_labels_folder, \"*.task-SLC.labels\")\n",
        "    # writing dev gold labels to file\n",
        "    with open(dev_labels_file, \"w\") as fout:\n",
        "      for article_id, sentence_id, prediction in zip(reference_articles_id, reference_sentence_id_list, dev_labels):\n",
        "        fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, prediction))\n",
        "  else: \n",
        "    reference_articles_id, reference_sentence_id_list, dev_labels = read_predictions_from_file(dev_labels_file)\n",
        "  if not are_ids_aligned(dev_article_id_list, dev_sentence_id_list, reference_articles_id, reference_sentence_id_list):\n",
        "    sys.exit(\"Exiting: development set article ids and gold labels are not aligned\")\n",
        "    \n",
        "  # computing the predictions on the development set\n",
        "  dev = np.array([ len(sentence) for sentence in dev_sentence_list ]).reshape(-1, 1)\n",
        "  predictions = model.predict(dev)\n",
        "\n",
        "  # writing predictions to file\n",
        "  with open(task_SLC_output_file, \"w\") as fout:\n",
        "    for article_id, sentence_id, prediction in zip(dev_article_id_list, dev_sentence_id_list, predictions):\n",
        "       print(\"%s\\t%s\\t%s\" % (article_id, sentence_id, prediction))\n",
        "       fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, prediction))\n",
        "  print(\"Predictions written to file \" + task_SLC_output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ7I6k2g4ZM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LogisticRegression(penalty='l2', class_weight='balanced', solver=\"lbfgs\")\n",
        "model.fit(X_train, gold_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KtELy5e5BOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict(X_dev)\n",
        "\n",
        "# writing predictions to file\n",
        "with open('perspective_output', \"w\") as fout:\n",
        "  for article_id, sentence_id, prediction in zip(dev_article_id_list, dev_sentence_id_list, predictions):\n",
        "     fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, 'propaganda' if prediction > 0.5 else 'non-propaganda'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryy7ffYx5ZQv",
        "colab_type": "text"
      },
      "source": [
        "# SLC task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ2VU6o6317Q",
        "colab_type": "text"
      },
      "source": [
        "## BERT\n",
        "\n",
        "Modified from https://www.kaggle.com/yuval6967/toxic-bert-plain-vanila"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWVjnHe2mQaE",
        "colab_type": "text"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0iIQvYD8OW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir working\n",
        "!wget https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
        "!unzip wwm_uncased_L-24_H-1024_A-16.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YWbYXknmMl2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_MODEL_PATH = 'wwm_uncased_L-24_H-1024_A-16/'\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = max([len(sentence.split(' ')) for sentence in sentence_list])\n",
        "SEED = 1234\n",
        "WORK_DIR = \"working/\"\n",
        "num_to_load=int(len(datadf)/10 * 9)                       #Train size to match time limit\n",
        "valid_size= len(datadf) - num_to_load                          #Validation Size\n",
        "EPOCHS = 1\n",
        "fold = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jujbHbY489_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Translate model from tensorflow to pytorch\n",
        "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
        "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
        "BERT_MODEL_PATH + 'bert_config.json',\n",
        "WORK_DIR + 'pytorch_model.bin')\n",
        "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6v8hfd4_Zx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_config = BertConfig('wwm_uncased_L-24_H-1024_A-16/'+'bert_config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEI_2nYQ_hS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the lines to BERT format\n",
        "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
        "def convert_lines(example, max_seq_length,tokenizer):\n",
        "    max_seq_length -=2\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    for text in tqdm(example):\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        if len(tokens_a)>max_seq_length:\n",
        "            tokens_a = tokens_a[:max_seq_length]\n",
        "            longer += 1\n",
        "        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n",
        "        all_tokens.append(one_token)\n",
        "    print(longer)\n",
        "    return np.array(all_tokens)\n",
        "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8ACp23a2XzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the lines to BERT format\n",
        "# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n",
        "def get_unigrams_and_bigrams(example, max_seq_length,tokenizer):\n",
        "    max_seq_length -=2\n",
        "    all_tokens = []\n",
        "    longer = 0\n",
        "    unigrams = {}\n",
        "    bigrams = {}\n",
        "    for text in tqdm(example):\n",
        "        tokens_a = tokenizer.tokenize(text)\n",
        "        previous = None\n",
        "        for token in tokens_a:\n",
        "          if not(token in unigrams):\n",
        "            unigrams[token] = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+[token]+[\"[SEP]\"])+[0] * (max_seq_length - 1)\n",
        "          if not(previous == None):\n",
        "            if not((previous, token) in bigrams):\n",
        "              bigrams[(previous, token)] = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+[previous, token]+[\"[SEP]\"])+[0] * (max_seq_length - 2)\n",
        "          previous = token\n",
        "    return unigrams, bigrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm1ttpTKfuGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = datadf\n",
        "dev_df = dev_datadf\n",
        "all_df = pd.concat([datadf, dev_datadf, test_datadf], axis=0)\n",
        "print('loaded %d records' % len(train_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSqJb1Dq3WjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams, bigrams = get_unigrams_and_bigrams(all_df['text'].fillna('DUMMY_VALUE'),MAX_SEQUENCE_LENGTH,tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGpvrRDgfx2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make sure all comment_text values are strings\n",
        "train_df['text'] = train_df['text'].astype(str) \n",
        "dev_df['text'] = dev_df['text'].astype(str)\n",
        "\n",
        "# Random shuffle train list\n",
        "train_df = train_df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcUwIB2nfzko",
        "colab_type": "code",
        "outputId": "514dbc0b-3524-4889-d7a4-f92abe2e6e3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sequences = convert_lines(train_df[\"text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
        "train_df=train_df.fillna(0)\n",
        "\n",
        "dev_sequences = convert_lines(dev_df[\"text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\n",
        "dev_df=dev_df.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16956/16956 [00:06<00:00, 2779.29it/s]\n",
            " 16%|█▌        | 347/2235 [00:00<00:00, 3464.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9841\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2235/2235 [00:00<00:00, 2993.52it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1284\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEKTLPkrf3Qd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df['target'] = train_df['label']\n",
        "y_columns=['target']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmhcuZTyZPqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if type(fold) is int and (fold < 9):\n",
        "  X = np.concatenate((sequences[:num_to_load-valid_size*fold],\n",
        "                     sequences[-valid_size*fold:]), axis=0)\n",
        "  y = np.concatenate((train_df[y_columns].values[:num_to_load-valid_size*fold],\n",
        "                      train_df[y_columns].values[-valid_size*fold:]),\n",
        "                     axis=0)\n",
        "\n",
        "  X_val = sequences[num_to_load-valid_size*fold:-valid_size*fold]                \n",
        "  y_val = train_df[y_columns].values[num_to_load-valid_size*fold:-valid_size*fold]\n",
        "\n",
        "  X_test = dev_sequences\n",
        "if fold == 0 or (fold == 'all'):\n",
        "  X = sequences[:num_to_load]                \n",
        "  y = train_df[y_columns].values[:num_to_load]\n",
        "  X_val = sequences[num_to_load:]                \n",
        "  y_val = train_df[y_columns].values[num_to_load:]\n",
        "  X_test = dev_sequences \n",
        "if fold == 9:\n",
        "  X = sequences[-valid_size*fold:]                \n",
        "  y = train_df[y_columns].values[-valid_size*fold:]\n",
        "  X_val = sequences[:-valid_size*fold:]                \n",
        "  y_val = train_df[y_columns].values[:-valid_size*fold]\n",
        "  X_test = dev_sequences "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoV8cjZJ3-kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = train_df.drop(['text'],axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0WpK_phX_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "lr=1e-5\n",
        "accumulation_steps=2\n",
        "alpha = 5\n",
        "EPOCHS = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9BSOKRHmfDC",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymyLkdm-4SQP",
        "colab_type": "code",
        "outputId": "da5eb508-f690-418c-dbf8-1485b74bd983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_df=train_df.head(num_to_load)\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long),\n",
        "                                               torch.tensor(y,dtype=torch.float))\n",
        "\n",
        "\n",
        "\n",
        "output_model_file = \"bert_pytorch_{}_{}_{}_{}_{}_{}_{}.bin\".format(lr, accumulation_steps,\n",
        "                                                                  EPOCHS, SEED, alpha, fold,\n",
        "                                                                  MAX_SEQUENCE_LENGTH)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(WORK_DIR,cache_dir=None,\n",
        "                                                      num_labels=len(y_columns))\n",
        "model.zero_grad()\n",
        "model = model.to('cuda')\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "train = train_dataset\n",
        "\n",
        "num_train_optimization_steps = int(EPOCHS*len(train)/batch_size/accumulation_steps)\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=lr,\n",
        "                     warmup=0.05,\n",
        "                     t_total=num_train_optimization_steps)\n",
        "\n",
        "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "model=model.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ff44b068890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvmBN9HI4l2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positives = float(sum(y))\n",
        "total = float(len(y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYOLjCwXK2wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tq = tqdm(range(EPOCHS))\n",
        "for ind, epoch in enumerate(tq):\n",
        "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "    avg_loss = 0.\n",
        "    avg_accuracy = 0.\n",
        "    lossf=None\n",
        "    tk0 = tqdm(enumerate(train_loader),\n",
        "                        total=len(train_loader),leave=False)\n",
        "    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n",
        "    tp = 0.\n",
        "    fp = 0.\n",
        "    fn = 0.\n",
        "    tn = 0.\n",
        "\n",
        "    pos_weight = [positives / (total - positives)*alpha]\n",
        "    for i,(x_batch, y_batch) in tk0:\n",
        "#        optimizer.zero_grad()\n",
        "        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
        "\n",
        "        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device), pos_weight=torch.FloatTensor(pos_weight).to(device))\n",
        "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "            scaled_loss.mean().backward()\n",
        "        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
        "            optimizer.step()                            # Now we can do an optimizer step\n",
        "            optimizer.zero_grad()\n",
        "        if lossf:\n",
        "            lossf = 0.98*lossf+0.02*loss.item()\n",
        "        else:\n",
        "            lossf = loss.item()\n",
        "        avg_loss += loss.item() / len(train_loader)\n",
        "        tp += torch.sum((torch.sigmoid(y_pred[:,0])>0.5) *\n",
        "        ((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device))).item()\n",
        "        fp += torch.sum((torch.sigmoid(y_pred[:,0])>0.5) *\n",
        "        ((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]<0.5).to(device))).item()\n",
        "        fn += torch.sum((torch.sigmoid(y_pred[:,0])<0.5) *\n",
        "                        ((torch.sigmoid(y_pred[:,0])<0.5) == (y_batch[:,0]>0.5).to(device))).item()\n",
        "        tn += torch.sum((torch.sigmoid(y_pred[:,0])<0.5) *\n",
        "                        ((torch.sigmoid(y_pred[:,0])<0.5) == (y_batch[:,0]<0.5).to(device))).item()\n",
        "        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
        "        tk0.set_postfix(loss = lossf, tp=tp, fn=fn)\n",
        "        \n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    print(precision, recall, avg_accuracy)\n",
        "    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), output_model_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-gwSEUSmjFz",
        "colab_type": "text"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRu7SVqI4Ujq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run validation\n",
        "# The following 3 lines are not needed but show how to download the model for prediction\n",
        "valid_model = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\n",
        "valid_model = torch.nn.DataParallel(valid_model)\n",
        "valid_model.load_state_dict(torch.load(output_model_file ))\n",
        "\n",
        "valid_model.to(device)\n",
        "for param in valid_model.parameters():\n",
        "    param.requires_grad=False\n",
        "valid_model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9fLuGgkzVJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_preds = np.zeros((len(X_val)))\n",
        "valid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\n",
        "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "tk0 = tqdm_notebook(valid_loader)\n",
        "    \n",
        "for i,(x_batch,)  in enumerate(tk0):\n",
        "    pred = valid_model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
        "    pred = torch.sigmoid(pred)\n",
        "    valid_preds[i*batch_size:(i+1)*batch_size]=pred[:,0].detach().cpu().squeeze().numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWtIRmx_plcg",
        "colab_type": "code",
        "outputId": "43378e1e-d7d0-4b23-9cdb-efec8296c6bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "tp = np.sum((valid_preds>0.5) * ((valid_preds>0.5) == (y_val.flatten()>0.5)))\n",
        "fp = np.sum((valid_preds>0.5) * ((valid_preds>0.5) == (y_val.flatten()<0.5)))\n",
        "fn = np.sum((valid_preds<0.5) * ((valid_preds<0.5) == (y_val.flatten()>0.5)))\n",
        "tn = np.sum((valid_preds<0.5) * ((valid_preds<0.5) == (y_val.flatten()<0.5)))\n",
        "  \n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (recall * precision) / (recall + precision)\n",
        "print(precision, recall, f1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.48015122873345933 0.5427350427350427 0.5095285857572718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RF89iFlmo6e",
        "colab_type": "text"
      },
      "source": [
        "### Get result on testset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWAGMbPt4RRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr=1e-5\n",
        "accumulation_steps=2\n",
        "alpha = 5\n",
        "EPOCHS = 1\n",
        "SEED = 1234\n",
        "batch_size=32\n",
        "bert_config = BertConfig('wwm_uncased_L-24_H-1024_A-16/'+'bert_config.json')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3mlK0TIp69-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_preds = np.zeros((len(X_test)))\n",
        "folds = 10\n",
        "\n",
        "for fold in range(folds):\n",
        "  output_model_file = \"bert_pytorch_{}_{}_{}_{}_{}_{}.bin\".format(lr, accumulation_steps,\n",
        "                                                                EPOCHS, SEED, alpha, fold)\n",
        "  _# Run validation\n",
        "  # The following 3 lines are not needed but show how to download the model for prediction\n",
        "  valid_model = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\n",
        "  valid_model = torch.nn.DataParallel(valid_model)\n",
        "  valid_model.load_state_dict(torch.load(output_model_file ))\n",
        "\n",
        "  valid_model.to(device)\n",
        "  for param in valid_model.parameters():\n",
        "    param.requires_grad=False\n",
        "  valid_model.eval()\n",
        "\n",
        "  test = torch.utils.data.TensorDataset(torch.tensor(X_test,dtype=torch.long))\n",
        "  test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  tk0 = tqdm_notebook(test_loader)\n",
        "    \n",
        "  for i,(x_batch,)  in enumerate(tk0):\n",
        "    pred = valid_model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
        "    pred = torch.sigmoid(pred)\n",
        "    test_preds[i*batch_size:(i+1)*batch_size]+=pred[:,0].detach().cpu().squeeze().numpy() / folds\n",
        "\n",
        "output_labels = ['non-propaganda', 'propaganda']\n",
        "with open(task_SLC_output_file, \"w\") as fout:\n",
        "  for article_id, sentence_id, prediction in zip(dev_article_id_list, dev_sentence_id_list, test_preds):\n",
        "    fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, output_labels[int(prediction > 0.5)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hv-kRLIxvyKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(X_test, num_labels=1):\n",
        "  test_preds = np.zeros((len(X_test)))\n",
        "  folds = 10\n",
        "\n",
        "  for fold in range(folds):\n",
        "    output_model_file = \"bert_pytorch_{}_{}_{}_{}_{}_{}.bin\".format(lr, accumulation_steps,\n",
        "                                                                EPOCHS, SEED, alpha, fold)\n",
        "    # load model\n",
        "    valid_model = BertForSequenceClassification(bert_config,num_labels=num_labels)\n",
        "    valid_model = torch.nn.DataParallel(valid_model)\n",
        "    valid_model.load_state_dict(torch.load(output_model_file ))\n",
        "\n",
        "    valid_model.to(device)\n",
        "    for param in valid_model.parameters():\n",
        "      param.requires_grad=False\n",
        "    valid_model.eval()\n",
        "\n",
        "    test = torch.utils.data.TensorDataset(torch.tensor(X_test,dtype=torch.long))\n",
        "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    tk0 = tqdm_notebook(test_loader)\n",
        "    \n",
        "    for i,(x_batch,)  in enumerate(tk0):\n",
        "      pred = valid_model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n",
        "      pred = torch.sigmoid(pred)\n",
        "      test_preds[i*batch_size:(i+1)*batch_size]+=pred[:,0].detach().cpu().squeeze().numpy() / folds\n",
        "  return test_preds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mO8L94CuFxq",
        "colab_type": "text"
      },
      "source": [
        "### Unigram and Bigram Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj0iDcT35spt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unigrams_and_bigrams(tokens, num_labels=1):\n",
        "\n",
        "  ind = 0\n",
        "  X_test = []\n",
        "  mapping = {}\n",
        "  for token, seq in tokens.items():\n",
        "    X_test.append(seq)\n",
        "    mapping[ind] = token\n",
        "    ind +=1\n",
        "\n",
        "    \n",
        "  test_preds = run_model(X_test, num_labels)\n",
        "  \n",
        "  res = []\n",
        "  for ind, score in enumerate(test_preds):\n",
        "    res.append((score, mapping[ind]))\n",
        "  \n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzOHycx-vfvg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_scores = run_model(test_seqs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhJgCF_26btd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigram_scores = unigrams_and_bigrams(unigrams)\n",
        "bigram_scores = unigrams_and_bigrams(bigrams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGSTd5NfvnlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigram_scores = sorted(unigram_scores)\n",
        "bigram_scores = sorted(bigram_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jkhOGlRCukP",
        "colab_type": "code",
        "outputId": "27f06ae7-ee06-4a1a-f06b-3cc662fc18c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "','.join([u[1] for u in unigram_scores[-20:]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'devastating,cruel,vile,irrational,absurd,brutal,vicious,stupid,coward,awful,ignorant,unbelievable,doomed,idiot,terrifying,disgusting,horrible,hideous,horrific,pathetic'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Mautf5STYL",
        "colab_type": "code",
        "outputId": "e137f924-bf96-43f0-b0e9-ae0da51359a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "','.join([' '.join(u[1]) for u in bigram_scores[-20:]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'shame ##less,totally insane,a horrible,utterly unacceptable,hysterical nonsense,the horrible,this horrific,absolutely disgusting,monumental stupidity,a pathetic,a disgusting,absolutely worthless,truly disgusting,utterly insane,this murderous,incredibly stupid,monstrous fraud,this lunatic,a disgrace,a hideous'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZi397SG63gh",
        "colab_type": "text"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Baseline for Task SLC\n",
        "\n",
        "Our baseline uses a logistic regression classifier on one feature only: the length of the sentence.\n",
        "\n",
        "Requirements: sklearn, numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZTESAZV7Akg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_main(train_folder, train_labels_folder, task_SLC_output_file,\n",
        "               dev_labels_folder=None, dev_labels_file=None):\n",
        "  # loading articles' content from *.txt files in the train folder\n",
        "  train_article_ids, train_sentence_ids, sentence_list = read_articles_from_file_list(train_folder)\n",
        "\n",
        "  # loading gold labels, articles ids and sentence ids from files *.task-SLC.labels in the train labels folder \n",
        "  reference_articles_id, reference_sentence_id_list, gold_labels = read_predictions_from_file_list(train_labels_folder, \"*.task-SLC.labels\")\n",
        "\n",
        "  # checking that the number of sentences in the raw training set and the gold label file\n",
        "  if not are_ids_aligned(train_article_ids, train_sentence_ids, reference_articles_id, reference_sentence_id_list):\n",
        "    sys.exit(\"Exiting: training set article ids and gold labels are not aligned\")\n",
        "  print(\"Loaded %d sentences from %d articles\" % (len(sentence_list), len(set(train_article_ids))))\n",
        "\n",
        "  # compute one feature for each sentence: the length of the sentence and train the model\n",
        "  train = np.array([ len(sentence) for sentence in sentence_list ]).reshape(-1, 1)\n",
        "  model = LogisticRegression(penalty='l2', class_weight='balanced', solver=\"lbfgs\")\n",
        "  model.fit(train, gold_labels)\n",
        "\n",
        "  # reading data from the development set\n",
        "  dev_article_id_list, dev_sentence_id_list, dev_sentence_list = read_articles_from_file_list(dev_folder)\n",
        "  if dev_labels_folder is not None:\n",
        "    assert(dev_labels_file is not None)\n",
        "    reference_articles_id, reference_sentence_id_list, dev_labels = read_predictions_from_file_list(dev_labels_folder, \"*.task-SLC.labels\")\n",
        "    # writing dev gold labels to file\n",
        "    with open(dev_labels_file, \"w\") as fout:\n",
        "      for article_id, sentence_id, prediction in zip(reference_articles_id, reference_sentence_id_list, dev_labels):\n",
        "        fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, prediction))\n",
        "  else: \n",
        "    reference_articles_id, reference_sentence_id_list, dev_labels = read_predictions_from_file(dev_labels_file)\n",
        "  if not are_ids_aligned(dev_article_id_list, dev_sentence_id_list, reference_articles_id, reference_sentence_id_list):\n",
        "    sys.exit(\"Exiting: development set article ids and gold labels are not aligned\")\n",
        "    \n",
        "  # computing the predictions on the development set\n",
        "  dev = np.array([ len(sentence) for sentence in dev_sentence_list ]).reshape(-1, 1)\n",
        "  predictions = model.predict(dev)\n",
        "\n",
        "  # writing predictions to file\n",
        "  with open(task_SLC_output_file, \"w\") as fout:\n",
        "    for article_id, sentence_id, prediction in zip(dev_article_id_list, dev_sentence_id_list, predictions):\n",
        "       print(\"%s\\t%s\\t%s\" % (article_id, sentence_id, prediction))\n",
        "       fout.write(\"%s\\t%s\\t%s\\n\" % (article_id, sentence_id, prediction))\n",
        "  print(\"Predictions written to file \" + task_SLC_output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka23DOiTBlrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split and evaluate\n",
        "\n",
        "train_folder = \"datasets/train-train-articles\" \n",
        "dev_folder = \"datasets/train-dev-articles\"\n",
        "train_labels_folder = \"datasets/train-train-labels-SLC\"\n",
        "task_SLC_output_file = \"baseline-output-SLC-eval.txt\"\n",
        "\n",
        "train_main(train_folder, train_labels_folder, task_SLC_output_file,\n",
        "               dev_labels_folder=\"datasets/train-dev-labels-SLC/\", dev_labels_file=\"dev-labels-SLC\")\n",
        "%run tools/task-SLC_scorer -s baseline-output-SLC-eval.txt -r dev-labels-SLC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWhwYJlbQdQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate Answer on development set\n",
        "\n",
        "train_folder = \"datasets/train-articles\" \n",
        "dev_folder = \"datasets/dev-articles\"\n",
        "train_labels_folder = \"datasets/train-labels-SLC\"\n",
        "task_SLC_output_file = \"baseline-output-SLC.txt\"\n",
        "\n",
        "train_main(train_folder, train_labels_folder, task_SLC_output_file,\n",
        "               dev_labels_file=\"datasets/dev.template-output-SLC.out\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8-py8T5lvrj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDjdkYhv65TM",
        "colab_type": "text"
      },
      "source": [
        "# FLC task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYvvKgTlQm66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev_folder = \"datasets/dev-articles\"\n",
        "propaganda_techniques_file = \"tools/data/propaganda-techniques-names.txt\"\n",
        "task_FLC_output_file = \"baseline-output-FLC.txt\"\n",
        "\n",
        "random.seed(10) # to make runs deterministic\n",
        "\n",
        "# loading articles' content from *.txt files in the dev folder\n",
        "file_list = glob.glob(os.path.join(dev_folder, \"*.txt\"))\n",
        "articles_content, articles_id = ([], [])\n",
        "for filename in file_list:\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        articles_content.append(f.read())\n",
        "        articles_id.append(os.path.basename(filename).split(\".\")[0][7:])\n",
        "\n",
        "with open(propaganda_techniques_file, \"r\") as f:\n",
        "    propaganda_techniques_names = [ line.rstrip() for line in f.readlines() ]\n",
        "\n",
        "with open(task_FLC_output_file, \"w\") as fout:\n",
        "    for article_content, article_id in zip(articles_content, articles_id):\n",
        "        start_fragment, end_fragment, article_length = (0, 0, len(article_content))\n",
        "        current_article_annotations = []\n",
        "        while end_fragment < article_length:\n",
        "            if end_fragment > 0:\n",
        "                technique_name = propaganda_techniques_names[random.randint(0, len(propaganda_techniques_names)-1)]\n",
        "                # check that there is no other annotation for the same anrticle and technique that overlaps\n",
        "                intersection_length = 0\n",
        "                if len(current_article_annotations) > 0:\n",
        "                    span_annotation = set(range(start_fragment, end_fragment))\n",
        "                    intersection_length = sum( [ len(span_annotation.intersection(previous_fragment))\n",
        "                             for previous_technique, previous_fragment in current_article_annotations \n",
        "                             if previous_technique==technique_name ])\n",
        "                if len(current_article_annotations) == 0 or intersection_length > 0:\n",
        "                    fout.write(\"%s\\t%s\\t%s\\t%s\\n\" % (article_id, technique_name, start_fragment, end_fragment))\n",
        "                    current_article_annotations.append((technique_name, set(range(start_fragment, end_fragment))))\n",
        "            start_fragment += random.randint(0, max(1, article_length-start_fragment))\n",
        "            end_fragment = min(start_fragment + random.randint(1,25), article_length)\n",
        "        print(\"article %s: added %d fragments\" % (article_id, len(current_article_annotations)))    \n",
        "\n",
        "print(\"Predictions written to file \" + task_FLC_output_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tl6MwI9QuML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%run tools/task-FLC_scorer -s baseline-output-FLC.txt -r tools/data/FLC-sample-labels -t tools/data/propaganda-techniques-names.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raoLi6tGQ237",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}